{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "720aceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets.synth import ConceptDriftStream, RandomRBFDrift, RandomRBF, RandomTree\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "930046f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300000it [2:03:44, 40.41it/s]\n",
      "300000it [1:26:49, 57.59it/s]\n",
      "300000it [2:34:34, 32.35it/s]\n"
     ]
    }
   ],
   "source": [
    "## the goal is to get a synthetic dataset close to the first one (of the ten) in the paper \n",
    "## I first create three datasets for each of the generators\n",
    "rbf = RandomRBF(seed_model=1, seed_sample=1, n_classes=5, n_features=21)\n",
    "rbfdrift = RandomRBFDrift(seed_model=1, seed_sample=1, n_classes=5, n_features=21)\n",
    "tree = RandomTree(seed_tree=1, seed_sample=1, n_classes=5, n_num_features=21, n_cat_features=0)\n",
    "\n",
    "n_iterations_1 =300000 ## Will have to be increased to obatin the desired number of data points.\n",
    "n_iterations_2 =300000 ## Will have to be increased to obatin the desired number of data points.\n",
    "n_iterations_3 =300000 ## Will have to be increased to obatin the desired number of data points.\n",
    "\n",
    "# stream1 = ConceptDriftStream(stream=rbf, drift_stream=rbfdrift, seed=42, position=int(n_iterations/3), width=10)\n",
    "# stream2 = ConceptDriftStream(stream=stream1, drift_stream=tree, seed=42, position=int(2*n_iterations/3), width=15)\n",
    "\n",
    "df1 = pd.DataFrame({'0' : [], '1' : [], '2' : [], '3' : [], '4' : [], '5' : [], '6' : [], '7' : [], '8' : [], '9' : [], '10' : [], '11' : [], '12' : [], '13' : [], '14' : [], '15' : [], '16' : [], '17' : [], '18' : [], '19' : [], '20' : [], 'label' : []})\n",
    "df2 = pd.DataFrame({'0' : [], '1' : [], '2' : [], '3' : [], '4' : [], '5' : [], '6' : [], '7' : [], '8' : [], '9' : [], '10' : [], '11' : [], '12' : [], '13' : [], '14' : [], '15' : [], '16' : [], '17' : [], '18' : [], '19' : [], '20' : [], 'label' : []})\n",
    "df3 = pd.DataFrame({'0' : [], '1' : [], '2' : [], '3' : [], '4' : [], '5' : [], '6' : [], '7' : [], '8' : [], '9' : [], '10' : [], '11' : [], '12' : [], '13' : [], '14' : [], '15' : [], '16' : [], '17' : [], '18' : [], '19' : [], '20' : [], 'label' : []})\n",
    "\n",
    "for x, y in tqdm(rbf.take(n_iterations_1)):\n",
    "    row = list(x.values()) + [y]\n",
    "    df1.loc[len(df1.index)] = row\n",
    "for x, y in tqdm(rbfdrift.take(n_iterations_2)):\n",
    "    row = list(x.values()) + [y]\n",
    "    df2.loc[len(df2.index)] = row\n",
    "for x, y in tqdm(tree.take(n_iterations_3)):\n",
    "    row = list(x.values()) + [y]\n",
    "    df3.loc[len(df3.index)] = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1ff43880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43114 35929 94984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    22147\n",
       "1.0    12608\n",
       "2.0    11431\n",
       "3.0     9132\n",
       "4.0     2700\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['label'] =  df['label'].astype(int)\n",
    "# m =len(df)\n",
    "# df1 = df[:(int(m/3))]\n",
    "# df2 = df[(int(m/3)):(int(2*m/3))]\n",
    "# df3 = df[(int(2*m/3)):]\n",
    "\n",
    "## Now, I try to imbalance those datasets with the weights given in the paper. \n",
    "## To do that, I just delete a part of the rows. \n",
    "## The main problem is that the initial datasets are not balanced at all and I often have to delete a lot of rows to obtain the good weights.\n",
    "## The more I delete, the more rows I have to generate in the first place. (and it depends on the type of generator)\n",
    "count1 = df1['label'].value_counts()\n",
    "count2 = df2['label'].value_counts()\n",
    "count3 = df3['label'].value_counts()\n",
    "\n",
    "class_weights_1 = {0: 1, 1: 4/5, 2: 3/5, 3: 2/5, 4: 1/5}\n",
    "class_weights_2 = {0: 1, 1: 5/10, 2: 5/10, 3: 4/10, 4: 1/10}\n",
    "class_weights_3 = {0: 1, 1: 5/10, 2: 5/10, 3: 4/10, 4: 1/10}\n",
    "## Here is a trick to delete as less rows as possible\n",
    "base_1 = min(count1[0], count1[1]*5/4, count1[2]*5/3, count1[3]*5/2, count1[4]*5)\n",
    "base_2 = min(count2[0], count2[1]*10/5, count2[2]*10/5, count2[3]*10/4, count2[4]*10/1)\n",
    "base_3 = min(count3[0], count3[1]*10/5, count3[2]*10/5, count3[3]*10/4, count3[4]*10/1)\n",
    "\n",
    "for class_label, weight in class_weights_1.items():\n",
    "    # Calculate the number of rows to keep for each class based on the weights\n",
    "    num_rows_to_keep = int(base_1 * weight)\n",
    "    \n",
    "    # Keep the desired number of rows for each class\n",
    "    df1[df1['label'] == class_label] = df1[df1['label'] == class_label].sample(n=num_rows_to_keep, random_state=42)\n",
    "for class_label, weight in class_weights_2.items():\n",
    "    # Calculate the number of rows to keep for each class based on the weight\n",
    "    num_rows_to_keep = int(base_2 * weight)\n",
    "    \n",
    "    # Keep the desired number of rows for each class\n",
    "    df2[df2['label'] == class_label] = df2[df2['label'] == class_label].sample(n=num_rows_to_keep, random_state=42)\n",
    "for class_label, weight in class_weights_3.items():\n",
    "    # Calculate the number of rows to keep for each class based on the weight\n",
    "    num_rows_to_keep = int(base_3 * weight)\n",
    "    \n",
    "    # Keep the desired number of rows for each class\n",
    "    df3[df3['label'] == class_label] = df3[df3['label'] == class_label].sample(n=num_rows_to_keep, random_state=42)\n",
    "    \n",
    "print(len(df1.dropna(axis=0)), len(df2.dropna(axis=0)), len(df3.dropna(axis=0)))\n",
    "df1 = df1.iloc[:100000] ## I keep the desired number of iterations (given in the paper)\n",
    "df2 = df2.iloc[:100000] ## I keep the desired number of iterations (given in the paper)\n",
    "df3 = df3.iloc[:100000] ## I keep the desired number of iterations (given in the paper)\n",
    "df_weighted = pd.concat([df1, df2, df3])\n",
    "df_weighted = df_weighted.dropna(axis=0)\n",
    "df_weighted['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b51e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
